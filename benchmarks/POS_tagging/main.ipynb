{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../POS_tagging/\")\n",
    "sys.path.append(\"../../../pomegranate/\")\n",
    "sys.path.append(r\"../../../WhoCell\\pomegranate\")\n",
    "sys.path.append(r\"../../../WhoCell\")\n",
    "sys.path.append(r\"../../../WhoCell/\")\n",
    "sys.path.append(r\"../../../WhoCell/who_cell/\")\n",
    "sys.path.append(r\"../../../WhoCell/who_cell/models/\")\n",
    "sys.path.append(r\"../../../WhoCell/who_cell/simulation/\")\n",
    "sys.path.append(r\"../../../WhoCell/who_cell/experiments/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "from IPython.core.display import HTML\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict\n",
    "from helpers import show_model, Dataset\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution\n",
    "\n",
    "from simulation_for_gibbs import Simulator_for_Gibbs\n",
    "from gibbs_sampler import GibbsSampler\n",
    "from experiment_report import ExperimentReport \n",
    "from gibbs_experiments import GibbsExperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"shell_port\": 51213,\n",
      "  \"iopub_port\": 58249,\n",
      "  \"stdin_port\": 53021,\n",
      "  \"control_port\": 60349,\n",
      "  \"hb_port\": 54237,\n",
      "  \"ip\": \"127.0.0.1\",\n",
      "  \"key\": \"fc8aec71-7cb5d0a420e5d7325ba3df38\",\n",
      "  \"transport\": \"tcp\",\n",
      "  \"signature_scheme\": \"hmac-sha256\",\n",
      "  \"kernel_name\": \"\"\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> jupyter <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> jupyter <app> --existing kernel-636e4c11-0ec3-41ec-bb4b-22f2c7920f19.json\n",
      "or even just:\n",
      "    $> jupyter <app> --existing\n",
      "if this is the most recent Jupyter kernel you have started.\n"
     ]
    }
   ],
   "source": [
    "%connect_info\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load and rearrange data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_path = \"data/tags-universal.txt\"\n",
    "data_path = \"data/brown-universal.txt\"\n",
    "\n",
    "data = Dataset(tags_path, data_path, train_test_split=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_sentences = []\n",
    "count_of_pos_in_sentence = [] \n",
    "\n",
    "for sed_id,sentence in data.training_set : \n",
    "    words_no_punctuation = list(filter(lambda x: x not in string.punctuation ,sentence.words))\n",
    "    tags_no_punctuation = list(filter(lambda x: x not in string.punctuation ,sentence.tags))\n",
    "    length_of_sentences.append(len(words_no_punctuation))\n",
    "    count_of_pos_in_sentence.append(Counter(tags_no_punctuation).items())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram of length of sentences : \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#length_of_sentences\n",
    "plt.hist(length_of_sentences,bins=30)\n",
    "plt.title(\"length_of_sentences\")\n",
    "print(\"Histogram of length of sentences : \")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(length_of_sentences,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_of_pos_in_sentences = Counter(chain(*count_of_pos_in_sentence))\n",
    "max_count_of_pos = max(map(lambda x:x[1],counts_of_pos_in_sentences.keys())) \n",
    "set_of_pos = list(set(map(lambda x:x[0],counts_of_pos_in_sentences.keys())))\n",
    "_pos_to_idx = {_pos:i for i,_pos in enumerate(set_of_pos)}\n",
    "\n",
    "multiple_occurrence_matrix = np.zeros((max_count_of_pos,len(set_of_pos)))\n",
    "\n",
    "for mul_occ,count in counts_of_pos_in_sentences.items() : \n",
    "    multiple_occurrence_matrix[mul_occ[1]-1,_pos_to_idx[mul_occ[0]]] = count\n",
    "    \n",
    "multiple_occurrence_df = pd.DataFrame(data = multiple_occurrence_matrix,index = range(1,max_count_of_pos+1),columns=set_of_pos)\n",
    "\n",
    "sns.heatmap(multiple_occurrence_df.head(20))\n",
    "print(\"how unique are the pos ? x axis - number of sentences where the POS occurred x times : \")\n",
    "print(\"trimmed in x = 20 \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build emissions probabilites from training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_emms_counts = {} # word => POS mapping \n",
    "\n",
    "for _word,_pos in data.training_set.stream() : \n",
    "    if _word not in reverse_emms_counts.keys() : \n",
    "        reverse_emms_counts[_word] = {_pos:1}\n",
    "        continue\n",
    "    if _pos not in reverse_emms_counts[_word] : \n",
    "        reverse_emms_counts[_word][_pos] = 1\n",
    "        continue\n",
    "    reverse_emms_counts[_word][_pos] += 1\n",
    "\n",
    "\n",
    "n_of_pos_per_word = [len(poss.items()) for word,poss in reverse_emms_counts.items() ]\n",
    "print(\"we counted the number of possible pos per word : \")\n",
    "print(f\"ambiguous per word : {Counter(n_of_pos_per_word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emms_counts = {} # word => POS mapping \n",
    "\n",
    "for _word,_pos in data.training_set.stream() : \n",
    "    if _pos == '.' : continue\n",
    "    if _pos not in emms_counts.keys() : \n",
    "        emms_counts[_pos] = {_word:1}\n",
    "        continue\n",
    "    if _word not in emms_counts[_pos] : \n",
    "        emms_counts[_pos][_word] = 1\n",
    "        continue\n",
    "    emms_counts[_pos][_word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emms_probs = {word : {pos:(count/sum(poss.values())) for pos,count in poss.items()} for word,poss in emms_counts.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build starting probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_count = {pos:0 for pos in set_of_pos}\n",
    "for idx,sentence in data.training_set : \n",
    "    tags_no_punctuation = list(filter(lambda x: x not in string.punctuation ,sentence.tags))\n",
    "    if len(tags_no_punctuation) == 0 : continue\n",
    "    _tag = tags_no_punctuation[0]\n",
    "    start_count[_tag] +=1\n",
    "\n",
    "start_probs = {pos:count/sum(start_count.values()) for pos,count in  start_count.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build clean test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_relevent_words = string.punctuation+ '``'+'.'+'--'+\"''\"+','"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_relevent_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_words = [] \n",
    "test_set_tags = [] \n",
    "\n",
    "for idx,sentence in data.testing_set : \n",
    "    clean_tuples = [(word,tag) for word,tag in zip(sentence.words,sentence.tags) if word not in non_relevent_words]\n",
    "    if len(clean_tuples) < 2 : continue\n",
    "    test_set_words.append([word for word,tag in clean_tuples ])\n",
    "    test_set_tags.append([tag for word,tag in clean_tuples ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test data and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## known transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_count_tuples = Counter(chain(*[list(zip(sen,sen[1:])) for sen in test_set_tags]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_count_dict = {}\n",
    "count_sum = {}\n",
    "for (_from,_to),count in transitions_count_tuples.items():\n",
    "    if _from not in transitions_count_dict.keys() : \n",
    "        transitions_count_dict[_from] = {_to:count}\n",
    "        count_sum[_from] = count if _from not in count_sum.keys() else count_sum[_from] + count\n",
    "        continue\n",
    "    if _to not in transitions_count_dict[_from].keys():\n",
    "        transitions_count_dict[_from][_to] = count\n",
    "        count_sum[_from] = count if _from not in count_sum.keys() else count_sum[_from] + count\n",
    "        continue\n",
    "transitions_probs = {k:{kk:(vv/count_sum[k]) for kk,vv in v.items()} for k,v in transitions_count_dict.items()}  \n",
    "transitions_probs_df = pd.DataFrame(transitions_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_probs_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## build model as pome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _extrect_states_transitions_dict_from_pome_model( model, states=None):\n",
    "    if states is None:\n",
    "        states = model.get_params()['states']\n",
    "\n",
    "    edges = model.get_params()['edges']\n",
    "    transition_dict = {}\n",
    "    final_states = []\n",
    "    for e in edges:\n",
    "        if ('start' in states[e[0]].name):\n",
    "            continue\n",
    "        if ('end' in states[e[1]].name):\n",
    "            final_states.append(eval(states[e[0]].name))\n",
    "            continue\n",
    "\n",
    "        _from = states[e[0]].name\n",
    "        _to = states[e[1]].name\n",
    "        _weight = e[2]\n",
    "\n",
    "        if _from not in transition_dict.keys():\n",
    "            transition_dict[_from] = {_to: _weight}\n",
    "        else:\n",
    "            transition_dict[_from][_to] = _weight\n",
    "    return transition_dict, final_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "states_track = {}\n",
    "\n",
    "pome_model = HiddenMarkovModel()\n",
    "\n",
    "for pos,trans in emms_probs.items() : \n",
    "    dist = DiscreteDistribution(trans)\n",
    "    state = State(dist,pos)\n",
    "    \n",
    "    pome_model.add_state(state)\n",
    "    states_track[pos] = state\n",
    "\n",
    "n_states = len(states_track)\n",
    "for _from_pos,_from_s in states_track.items():\n",
    "    for _to_pos,_to_s in states_track.items():\n",
    "        if _to_pos in transitions_probs[_from_pos].keys():\n",
    "            pome_model.add_transition(_from_s,_to_s,transitions_probs[_from_pos][_to_pos])\n",
    "\n",
    "for _pos,_s in states_track.items():\n",
    "    pome_model.add_transition(pome_model.start,_s,start_probs[_pos])\n",
    "    \n",
    "pome_model.bake()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_set_words_no_unknowns = [[(word if word in data.training_set.vocab else None)  for word in sentence] for sentence in test_set_words]\n",
    "# pome_model.fit(test_set_words_no_unknowns,batches_per_epoch=5,n_jobs = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### build transitions from results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pome_transitions,_ = _extrect_states_transitions_dict_from_pome_model(pome_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# compare_transition(transitions_probs_df,pome_transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling the known HMM as test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def build_pome_for_pos_exp(trnasitions,start_probs,emms_probs) : \n",
    "\n",
    "    states_track = {}\n",
    "\n",
    "    pome_model = HiddenMarkovModel()\n",
    "\n",
    "    for pos,trans in emms_probs.items() : \n",
    "        dist = DiscreteDistribution(trans)\n",
    "        state = State(dist,pos)\n",
    "\n",
    "        pome_model.add_state(state)\n",
    "        states_track[pos] = state\n",
    "\n",
    "    n_states = len(states_track)\n",
    "    for _from_pos,_from_s in states_track.items():\n",
    "        for _to_pos,_to_s in states_track.items():\n",
    "            if _to_pos in trnasitions[_from_pos].keys():\n",
    "                pome_model.add_transition(_from_s,_to_s,trnasitions[_from_pos][_to_pos])\n",
    "\n",
    "    for _pos,_s in states_track.items():\n",
    "        pome_model.add_transition(pome_model.start,_s,start_probs[_pos])\n",
    "\n",
    "    pome_model.bake()\n",
    "    return pome_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     24,
     40,
     55,
     78
    ]
   },
   "outputs": [],
   "source": [
    "def run_pos_experiment(test_set_words,start_probs,emms_probs,number_of_iters,\n",
    "                       N,description,known_transitions,title,is_only_observed_stratgy,\n",
    "                       comper_transitions,comper_transitions_title,state_order_for_plot):\n",
    "    gs = GibbsSampler(2,5) if not is_only_observed_stratgy else GibbsSampler(2,5,transition_sampling_profile = 'observed')\n",
    "    sampled_transitions, ws, transitions,states_picked_by_w = gs.sample_known_emissions(test_set_words,start_probs,\n",
    "                                                                                                  emms_probs,number_of_iters,N=N)\n",
    "\n",
    "#     print(description)\n",
    "#     print(f\"the format is \\\"known / {comper_transitions_title}/{title} \\\") \")\n",
    "    \n",
    "#     if comper_transitions is None : \n",
    "#         compare_transition(known_transitions,transitions[-1])\n",
    "#     else : \n",
    "#         compare_transition(known_transitions,comper_transitions,transitions[-1])\n",
    "\n",
    "#     plot_w_dist(ws,test_set_words)\n",
    "\n",
    "#     plot_states_transitions_as_lines(known_transitions,transitions[-1],state_order_for_plot,f\"{title}\")\n",
    "    \n",
    "    res = {\"sampled_transitions\":sampled_transitions, \n",
    "            \"ws\":ws, \n",
    "            \"transitions\":transitions,\n",
    "            \"states_picked_by_w\":states_picked_by_w}\n",
    "    return res\n",
    "    \n",
    "def plot_w_dist(all_ws,test_set_words) : \n",
    "    samples_to_take = np.random.randint(0,len(all_ws),100)\n",
    "    max_sentence = max(map(len,[test_set_words[i] for i in  samples_to_take]))\n",
    "    max_sentence = max_sentence if max_sentence > N else N\n",
    "\n",
    "    w_matrix = np.ones((max_sentence,len(samples_to_take)))*-1 \n",
    "\n",
    "\n",
    "    for i,i_word in enumerate(samples_to_take) :\n",
    "        word = test_set_words[i_word]\n",
    "        w_matrix[0:len(word),i] =  0\n",
    "        w_matrix[all_ws[-1][i_word],i] =  1\n",
    "\n",
    "    sns.heatmap(w_matrix)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_states_transitions_as_lines(known,_compr,state_order_for_plot,title) : \n",
    "    fig,subs = plt.subplots(2,1)\n",
    "    _compr_df = pd.DataFrame(_compr)\n",
    "    known_df = pd.DataFrame(known)\n",
    "    \n",
    "    _compr_df = _compr_df[state_order_for_plot].loc[state_order_for_plot]\n",
    "    known_df = known_df[state_order_for_plot].loc[state_order_for_plot]\n",
    "    \n",
    "    known_df.plot(figsize=(8,8),ax=subs[0])\n",
    "    _compr_df.plot(figsize=(8,8),ax=subs[1])\n",
    "    \n",
    "    subs[0].set_title(\"known\")\n",
    "    subs[1].set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "def compare_transition(first_df,second_df,third_df = None):\n",
    "    if type(first_df) is dict : \n",
    "        first_df = pd.DataFrame(first_df)\n",
    "    if type(second_df) is dict : \n",
    "        second_df = pd.DataFrame(second_df)\n",
    "    \n",
    "    if third_df is not None : \n",
    "        if type(third_df) is dict :\n",
    "            third_df = pd.DataFrame(third_df)\n",
    "\n",
    "    second_df = second_df[first_df.columns]\n",
    "    second_df = second_df.loc[first_df.index]\n",
    "    \n",
    "    if third_df is not None : \n",
    "        third_df = third_df[first_df.columns]\n",
    "        third_df = third_df.loc[first_df.index]\n",
    "    \n",
    "    if third_df is not None :\n",
    "        return first_df.round(decimals=3).astype(str) +' / ' + second_df.round(decimals=3).astype(str)+' / ' + third_df.round(decimals=3).astype(str)\n",
    "        \n",
    "    comps_plot = first_df.round(decimals=3).astype(str) +' / ' + second_df.round(decimals=3).astype(str)\n",
    "    return comps_plot\n",
    "\n",
    "def plot_compersion(first_df_stack,second_df_stack,\n",
    "                    first_title = 'first',second_title = 'second',\n",
    "                    third_df_stack = None,third_title = 'third'):\n",
    "    if type(first_df_stack) is dict : \n",
    "        first_df_stack = pd.DataFrame(first_df_stack).stack()\n",
    "    if type(second_df_stack) is dict : \n",
    "        second_df_stack = pd.DataFrame(second_df_stack).stack()\n",
    "\n",
    "    if third_df_stack is not None : \n",
    "        if type(third_df_stack) is dict :\n",
    "            third_df_stack = pd.DataFrame(third_df_stack).stack()\n",
    "\n",
    "    if third_df_stack is None : \n",
    "        cobined_stack_df = pd.concat([first_df_stack,second_df_stack],axis=1).fillna(0)\n",
    "        cobined_stack_df = cobined_stack_df.rename(columns={0:first_title,1:second_title})\n",
    "        \n",
    "        \n",
    "        plt.scatter(cobined_stack_df[first_title],cobined_stack_df[second_title])\n",
    "        plt.xlabel(first_title)\n",
    "        plt.ylabel(f\"{second_title}\")\n",
    "    else : \n",
    "        cobined_stack_df = pd.concat([first_df_stack,second_df_stack,third_df_stack],axis=1).fillna(0)\n",
    "        cobined_stack_df = cobined_stack_df.rename(columns={0:first_title,1:second_title,2:third_title})\n",
    "\n",
    "\n",
    "        plt.scatter(cobined_stack_df[first_title],cobined_stack_df[second_title])\n",
    "        plt.scatter(cobined_stack_df[first_title],cobined_stack_df[third_title])\n",
    "        plt.legend([second_title,third_title])\n",
    "        plt.xlabel(first_title)\n",
    "        plt.ylabel(f\"{second_title}/{third_title}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = 0.5\n",
    "N= 40\n",
    "number_of_iters = 25\n",
    "\n",
    "few_observations =  Simulator_for_Gibbs.sample_traj_for_few_obs(pc,test_set_words)\n",
    "few_obs_test_set_words = few_observations[0]\n",
    "few_obs_test_set_ws = few_observations[1]\n",
    "\n",
    "state_order_for_plot = ['NOUN', 'DET', 'PRON', 'ADJ', 'ADP', 'VERB', 'ADV', 'NUM', 'X', 'PRT','CONJ']\n",
    "gs = GibbsSampler(2,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning from P(C) = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_experiment_args = {\"test_set_words\":test_set_words,\n",
    "\"start_probs\":start_probs,\n",
    "\"emms_probs\":emms_probs,\n",
    "\"number_of_iters\":number_of_iters,\n",
    "\"N\":2,\n",
    "\"description\":\"train naive over P(C) = 1 \",\n",
    "\"is_only_observed_stratgy\":False,\n",
    "\"known_transitions\":transitions_probs_df,\n",
    "\"comper_transitions\":None,\n",
    "\"comper_transitions_title\" :None,\n",
    "\"state_order_for_plot\":state_order_for_plot,\n",
    "\"title\":\"P(C) = 1\"}\n",
    "first_experiment_results = run_pos_experiment(**first_experiment_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pass the right length , and learn transitions when some observations are missing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_known_N = [len(seq) for seq in test_set_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_experiment_args = {\"test_set_words\":few_obs_test_set_words,\n",
    "\"start_probs\":start_probs,\n",
    "\"emms_probs\":emms_probs,\n",
    "\"number_of_iters\":number_of_iters,\n",
    "\"is_only_observed_stratgy\":False,\n",
    "\"N\":_known_N,\n",
    "\"description\":\"train naive over P(C) = 0.5 with known N \",\n",
    "\"known_transitions\":transitions_probs_df,\n",
    "\"comper_transitions\":None,\n",
    "\"comper_transitions_title\" :None,\n",
    "\"state_order_for_plot\":state_order_for_plot,\n",
    "\"title\":\"P(C) = 0.5,known N\"}\n",
    "second_experiment_results = run_pos_experiment(**second_experiment_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare to naive - learn using regular model on few observations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_experiment_args = {\"test_set_words\":few_obs_test_set_words,\n",
    "\"start_probs\":start_probs,\n",
    "\"emms_probs\":emms_probs,\n",
    "\"number_of_iters\":number_of_iters,\n",
    "\"N\":2,\n",
    "\"is_only_observed_stratgy\":False,\n",
    "\"description\":\"reguler over few observations \",\n",
    "\"known_transitions\":transitions_probs_df,\n",
    "\"comper_transitions\":None,\n",
    "\"comper_transitions_title\" :None,\n",
    "\"state_order_for_plot\":state_order_for_plot,\n",
    "\"title\":\"reguler\"}\n",
    "third_experiment_results = run_pos_experiment(**third_experiment_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## few observation known N sample only \"observed\" strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_experiment_args = {\"test_set_words\":few_obs_test_set_words,\n",
    "\"start_probs\":start_probs,\n",
    "\"emms_probs\":emms_probs,\n",
    "\"number_of_iters\":number_of_iters,\n",
    "\"N\":_known_N,\n",
    "\"is_only_observed_stratgy\":True,\n",
    "\"description\":\"few with known N and observed strategy \",\n",
    "\"known_transitions\":transitions_probs_df,\n",
    "\"comper_transitions\":None,\n",
    "\"comper_transitions_title\" :None,\n",
    "\"state_order_for_plot\":state_order_for_plot,\n",
    "\"title\":\"few only observed \"}\n",
    "fourth_experiment_results = run_pos_experiment(**fourth_experiment_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare all results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare experiments transitions using line plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = {}\n",
    "all_states[\"stationary\"] =  {state:count/sum(Counter(chain(*test_set_tags)).values()) for state,count  in Counter(chain(*test_set_tags)).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compre_transitions_line_plots(original_transitions,first_comprr,second_comprr,\n",
    "                                 states_to_comapre,stationary_transitions,order= ['NOUN', 'DET', 'PRON', 'ADJ', 'ADP', 'VERB', 'ADV', 'NUM', 'X', 'PRT',\n",
    "                                 'CONJ'],\n",
    "                                 first_comprr_name = \"regular\",second_comprr_name = 'with W') : \n",
    "\n",
    "    for state in states_to_comapre :\n",
    "        fig,sub = plt.subplots(1,figsize=(8,8))\n",
    "        \n",
    "        print(f\"{state} : \")\n",
    "        \n",
    "        pd.DataFrame(original_transitions)[state].loc[order].plot(ax = sub)\n",
    "        pd.DataFrame(first_comprr)[state].loc[order].plot(ax = sub)\n",
    "        pd.DataFrame(second_comprr)[state].loc[order].plot(ax = sub)\n",
    "        pd.DataFrame(stationary_transitions).loc[order].plot(ax = sub)\n",
    "        plt.legend(['known transitions',f'{first_comprr_name}',f'{second_comprr_name}',\"stationary\"])\n",
    "        plt.show()\n",
    "\n",
    "compre_transitions_line_plots(first_experiment_results['transitions'][-1],second_experiment_results['transitions'][-1],\n",
    "                              third_experiment_results['transitions'][-1],\n",
    "                             transitions_probs_df.columns.to_list(),all_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## per iter distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def extrect_observed_transitions(ws,_states_picked):\n",
    "#     real_walks = [] \n",
    "#     for w,states_walk in zip(ws,_states_picked) : \n",
    "#         real_transitions = [(states_walk[_f],states_walk[_t]) for _f,_t in zip(range(len(w)),range(1,len(w))) if (w[_t]-w[_f]) == 1]\n",
    "#         real_walks.append(real_transitions)\n",
    "\n",
    "#     real_transition_dict = {}\n",
    "#     for (_from,_to),count in  Counter(itertools.chain(*real_walks)).items() :\n",
    "#         if _from not in real_transition_dict.keys() :\n",
    "#             real_transition_dict[_from] = {_to:count}\n",
    "#         else : \n",
    "#             real_transition_dict[_from][_to] = count\n",
    "            \n",
    "#     real_transition_dict_normalized = {_from:{__from:_count/sum(to.values()) for __from,_count in to.items()} for _from,to in real_transition_dict.items()}\n",
    "#     return real_transition_dict_normalized\n",
    "\n",
    "# all_transitions_knownN_observed_only = [extrect_observed_transitions(ws_list,_states_picked) for ws_list,_states_picked in zip(all_ws_knownN,_states_picked_by_w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "er = ExperimentReport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transitions_results_list = [all_transitions,all_transitions_knownN,all_transitions_fixed_N,all_transitions_few_fixed_N,all_transitions_few_regular] \n",
    "# w_results_list = [all_ws,all_ws_knownN,all_ws_fixed_N,all_ws_few_fixed_N,all_ws_few_regular] \n",
    "# experiments_name_list = [\"P(C) = 1\" ,f\"P(C) = {pc} \\ right N as prior\",f\"P(C) =  1 , constent N={N} \",f\"P(C) =  {pc} , constent N \",f\"P(C) =  {pc} , regular hmm \"]\n",
    "# global_params = [pc,N] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_results_list = [first_experiment_results['transitions'],second_experiment_results['transitions'],third_experiment_results['transitions'],fourth_experiment_results['transitions']]\n",
    "w_results_list = [first_experiment_results['ws'],second_experiment_results['ws'],third_experiment_results['ws'],fourth_experiment_results['ws']]\n",
    "experiments_name_list = [first_experiment_args['title'],second_experiment_args['title'],third_experiment_args['title'],fourth_experiment_args['title']]\n",
    "global_params = [pc,N] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__l1_distance = lambda dist,state,known : abs(dist[state]-known) if state in dist.keys() else known\n",
    "__cross_entropy_distance = lambda dist,state,known : -1*known*np.log(dist[state]) if state in dist.keys() else (-1*known*np.log(0.0001))\n",
    "\n",
    "_l1_distance = lambda known_dist,comp_dist:sum(([__l1_distance(comp_dist,state,prob) for state,prob in known_dist.items()]))\n",
    "_cross_entropy_distance = lambda known_dist,comp_dist:sum([__cross_entropy_distance(comp_dist,state,prob) for state,prob in known_dist.items()])\n",
    "\n",
    "l1_distance = lambda known_trns,comp_trans : np.mean([_l1_distance(dist,comp_trans[state]) for state,dist in known_trns.items()])\n",
    "cross_entropy_distance = lambda known_trns,comp_trans : np.mean([_cross_entropy_distance(dist,comp_trans[state]) for state,dist in known_trns.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_trajectory_prob = lambda traj,model_trans:reduce(lambda x,y:x*y,[(model_trans[_f][_t] if _t in model_trans[_f].keys() else 0) for _f,_t in zip(traj,traj[1:])])\n",
    "\n",
    "original_model = build_pome_for_pos_exp(transitions_probs,start_probs,emms_probs)\n",
    "sampled_trajs = original_model.sample(n = 300,length = N,path=True)\n",
    "sampled_trajs_states = [[obs.name for obs in traj[1] if 'start' not in obs.name] for traj in sampled_trajs]\n",
    "known_probs = [_trajectory_prob(traj,transitions_probs) for traj in sampled_trajs_states]\n",
    "\n",
    "def kl_distance_transitions(comp_transitions,sampled_trajs_states,known_probs) : \n",
    "    comp_probs = [_trajectory_prob(traj,comp_transitions) for traj in sampled_trajs_states]\n",
    "    return sum([np.log(known/comp)*known for known,comp in zip(known_probs,comp_probs)])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "original_model = build_pome_for_pos_exp(transitions_probs,start_probs,emms_probs)\n",
    "\n",
    "external_samples = original_model.sample(300,length=70) \n",
    "all_kl_results = {}\n",
    "all_l1_results = {}\n",
    "all_ce_results = {}\n",
    "for _trans_list,exp_name in zip(transitions_results_list,experiments_name_list) : \n",
    "    kl_dist = [kl_distance_transitions(_t,sampled_trajs_states,known_probs) for _t in _trans_list]\n",
    "    l1_dist = [l1_distance(transitions_probs,_t) for _t in _trans_list]\n",
    "    ce_dist = [cross_entropy_distance(transitions_probs,_t) for _t in _trans_list]\n",
    "    \n",
    "    all_kl_results[exp_name] = kl_dist\n",
    "    all_l1_results[exp_name] = l1_dist\n",
    "    all_ce_results[exp_name] = ce_dist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_normalization = lambda dic: {k:{kk:vv/sum(v.values()) for kk,vv in v.items()} for k,v in dic.items()}\n",
    "\n",
    "kl_dist = [kl_distance_transitions(transition_normalization(_t.null_transitions_dict),sampled_trajs_states,known_probs) for _t in second_experiment_results['sampled_transitions']]\n",
    "l1_dist = [l1_distance(transitions_probs,transition_normalization(_t.null_transitions_dict)) for _t in second_experiment_results['sampled_transitions']]\n",
    "ce_dist = [cross_entropy_distance(transitions_probs,transition_normalization(_t.null_transitions_dict)) for _t in second_experiment_results['sampled_transitions']]\n",
    "\n",
    "all_kl_results[\"silent_trans\"] = kl_dist\n",
    "all_l1_results[\"silent_trans\"] = l1_dist\n",
    "all_ce_results[\"silent_trans\"] = ce_dist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for all_results,name in zip([all_kl_results,all_l1_results,all_ce_results],['kl','crossEntropy','l1']) : \n",
    "    fig, sub = plt.subplots(1,1,figsize=(8, 8))\n",
    "    model_results_df = pd.DataFrame(all_results)\n",
    "\n",
    "    sns.lineplot(data=model_results_df, ax=sub, legend='full', dashes=False)\n",
    "    sub.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    sub.set_title(f\"{name}\")\n",
    "    sub.set_xlabel(\"iter\")\n",
    "    sub.set_ylabel(f\"{name}\")\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.8)\n",
    "    plt.subplots_adjust(wspace=0.8)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_normalization = lambda dic: {k:{kk:vv/sum(v.values()) for kk,vv in v.items()} for k,v in dic.items()}\n",
    "\n",
    "null_to_all = [l1_distance(transitions_probs,transition_normalization(_t.null_transitions_dict)) for _t in second_experiment_results['sampled_transitions']]\n",
    "real_to_all = [l1_distance(transitions_probs,transition_normalization(_t.observed_transitions_dict)) for _t in second_experiment_results['sampled_transitions']]\n",
    "all_to_all = [l1_distance(transitions_probs,transition_normalization(_t)) for _t in second_experiment_results['sampled_transitions']]\n",
    "real_to_null = [l1_distance(transition_normalization(_t.null_transitions_dict),transition_normalization(_t.observed_transitions_dict)) for _t in second_experiment_results['sampled_transitions']]\n",
    "\n",
    "\n",
    "res = {\"not observed\":null_to_all,\"observed\" : real_to_all,\"both\":all_to_all,\"observed to not\":real_to_null}\n",
    "\n",
    "pd.DataFrame(res).plot(title = \"l1 to known transitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_normalization = lambda dic: {k:{kk:vv/sum(v.values()) for kk,vv in v.items()} for k,v in dic.items()}\n",
    "\n",
    "null_to_all = [l1_distance((transition_normalization(_t)),transition_normalization(_t.null_transitions_dict)) for _t in second_experiment_results['sampled_transitions']]\n",
    "real_to_all = [l1_distance((transition_normalization(_t)),transition_normalization(_t.observed_transitions_dict)) for _t in second_experiment_results['sampled_transitions']]\n",
    "\n",
    "\n",
    "res = {\"not observed\":null_to_all,\"observed\" : real_to_all}\n",
    "\n",
    "pd.DataFrame(res).plot(title = \"l1 to sampled transitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_normalization = lambda dic: {k:{kk:vv/sum(v.values()) for kk,vv in v.items()} for k,v in dic.items()}\n",
    "\n",
    "null_to_all = [l1_distance(transitions_probs,transition_normalization(_t.null_transitions_dict)) for _t in fourth_experiment_results['sampled_transitions']]\n",
    "real_to_all = [l1_distance(transitions_probs,transition_normalization(_t.observed_transitions_dict)) for _t in fourth_experiment_results['sampled_transitions']]\n",
    "all_to_all = [l1_distance(transitions_probs,transition_normalization(_t)) for _t in fourth_experiment_results['sampled_transitions']]\n",
    "real_to_null = [l1_distance(transition_normalization(_t.null_transitions_dict),transition_normalization(_t.observed_transitions_dict)) for _t in fourth_experiment_results['sampled_transitions']]\n",
    "\n",
    "\n",
    "res = {\"not observed\":null_to_all,\"observed\" : real_to_all,\"both\":all_to_all,\"observed to not\":real_to_null}\n",
    "\n",
    "pd.DataFrame(res).plot(title = \"l1 to known transitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_normalization = lambda dic: {k:{kk:vv/sum(v.values()) for kk,vv in v.items()} for k,v in dic.items()}\n",
    "\n",
    "null_to_all = [l1_distance((transition_normalization(_t)),transition_normalization(_t.null_transitions_dict)) for _t in fourth_experiment_results['sampled_transitions']]\n",
    "real_to_all = [l1_distance((transition_normalization(_t)),transition_normalization(_t.observed_transitions_dict)) for _t in fourth_experiment_results['sampled_transitions']]\n",
    "\n",
    "\n",
    "res = {\"not observed\":null_to_all,\"observed\" : real_to_all}\n",
    "\n",
    "pd.DataFrame(res).plot(title = \"l1 to sampled transitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_seq_of_tags = [[seq[i] for i in ws ] for ws,seq in zip(few_obs_test_set_ws,test_set_tags)]\n",
    "tags_from_sampler = second_experiment_results['ws'][-1]\n",
    "tags_random_allocation =[[seq[i] for i in np.random.randint(0,len(seq),size=len(ws)) ] for ws,seq in zip(few_obs_test_set_ws,test_set_tags)]\n",
    "\n",
    "distance_func =lambda a,b: sum(map(lambda x:int(x[0]!=x[1]) ,zip(a,b))) / len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dists = [distance_func(_rand,known) for _rand,known in  zip(tags_random_allocation,few_seq_of_tags)]\n",
    "sapmled_dists = [distance_func(_sampled,known) for _sampled,known in  zip(tags_from_sampler,few_seq_of_tags)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.hist(random_dists,bins=50)\n",
    "plt.hist(sapmled_dists,bins=50)\n",
    "plt.title(\"Normalized Hamming distance per sentence\")\n",
    "plt.legend([\"distance from random allocation\",\"distance from W\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tags_from_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
